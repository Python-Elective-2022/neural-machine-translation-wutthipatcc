{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPlCKTlKY9TS"
      },
      "source": [
        "This Notebook mostly follows along with [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)\n",
        "\n",
        "The main differences is that I used the [Attention layer built into keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) and used keras model.fit to train the model\n",
        "\n",
        "I found [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) useful for understanding Attention and Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfrvAt8lO2JO"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gjwv8PRRJkF"
      },
      "source": [
        "Check if using a GPU.  \n",
        "Turn on the GPU in Colab: 'Runtime' -> 'Change runtime type', select GPU in the dropdown menu   \n",
        "It is reccomended to leave the GPU off untill you need it for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIR3e8u2Kh-U"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CJFznnTNEW"
      },
      "source": [
        "# Data and input Pipline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5CZYhQIUoDm"
      },
      "source": [
        "Used [Portugese-English translation dataset](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en) from TensorFlow Datasets([tfds](https://www.tensorflow.org/datasets/api_docs/python/tfds))\n",
        "\n",
        "The dataset include other language pairs that you could try to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTtbkCETO6rz"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXFj2IWw566"
      },
      "source": [
        "[en.numpy() for pt, en in train_examples.take(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CIClJh9Wg38"
      },
      "source": [
        "Text tokenized using Byte Pair Encoding.\n",
        "\n",
        "https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8IcrwtrPGGZ"
      },
      "source": [
        "#####Heiko fix deprecated error message######\n",
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "  (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "#############\n",
        "\n",
        "# tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "#     (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "# tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "#     (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RthlX4VOPKgS"
      },
      "source": [
        "sample_string = 'Transformer is complicated.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8lYdZYcGDQ"
      },
      "source": [
        "Create a funtion to tokenize and add start and end tokens for both languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgUyNUe6QxFR"
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1_yUjIgdncT"
      },
      "source": [
        "This is needed to use .map  \n",
        "See https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline for more detail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0WxEmfESh6q"
      },
      "source": [
        "def tf_encode(pt, en):\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiiGOnpgQvGM"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa9MKksJe4Yu"
      },
      "source": [
        "Drop example where the tokenization of either language is greater then 40\n",
        "This is to keep things fast should work without this "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMBZugGJoG9i"
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hRshWw7oJ1C"
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPNG2fdpZG-"
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdu5CgbUtx0g"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "pt_batch, en_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udEXAB52fx6p"
      },
      "source": [
        "# Positional Encoding\n",
        "A vector added to the embedding to encode positional information\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/transformer#positional_encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_HhzsSNRFs8"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI-t0hWpRxGj"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAnFbTgIvU7h"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOueEfaSh9r6"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap95T6pfi0oP"
      },
      "source": [
        "Basic attention test. Compare results with  \n",
        "https://www.tensorflow.org/tutorials/text/transformer#scaled_dot_product_attention\n",
        "\n",
        "Shape = (batch_size , seq_length, depth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69RkcXS9ilII"
      },
      "source": [
        "query = tf.keras.layers.Input(shape=(None,3,))\n",
        "value = tf.keras.layers.Input(shape=(4,2,)) \n",
        "key = tf.keras.layers.Input(shape=(4,3,))\n",
        "\n",
        "x = tf.keras.layers.Attention()([query, value, key])\n",
        "model = tf.keras.models.Model(inputs=[query, value, key], outputs=x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6W3xVFG1kwf"
      },
      "source": [
        "temp_k = tf.constant([[[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "temp_q = tf.constant([[[0, 10, 0]]], dtype=tf.float32)  # (1, 3)\n",
        "model.predict([temp_q,temp_v,temp_k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TE96xLn15vK"
      },
      "source": [
        "temp_q = tf.constant([[[0, 0, 10]]], dtype=tf.float32)  # (1, 3)\n",
        "model.predict([temp_q,temp_v,temp_k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSW_wCMm2KET"
      },
      "source": [
        "temp_q = tf.constant([[[10, 10, 0]]], dtype=tf.float32)  # (1, 3)\n",
        "model.predict([temp_q,temp_v,temp_k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaLdbnPx2M18"
      },
      "source": [
        "temp_q = tf.constant([[[0, 0, 10], [0, 10, 0], [10, 10, 0]]], dtype=tf.float32)  # (3, 3)\n",
        "model.predict([temp_q,temp_v,temp_k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lwPLeF0mqZj"
      },
      "source": [
        "test of Attention with comparable shapes to the 2nd attention block in the transformer decoder (except not multi headed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsOoof95uUHd"
      },
      "source": [
        "enc = tf.keras.layers.Input(shape=(40,512,))\n",
        "tar = tf.keras.layers.Input(shape=(38,512,)) \n",
        "\n",
        "x = tf.keras.layers.Attention()([tar, enc, enc],mask = [None,None])\n",
        "model = tf.keras.models.Model(inputs=[tar, enc], outputs=x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKQrhS15MQQ"
      },
      "source": [
        "## Multi Headed Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QXu7IDrmGT7"
      },
      "source": [
        "Test of multi-headed Attention    \n",
        "Shape = (batch_size , num_heads, seq_length, depth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gPi8JJd731w"
      },
      "source": [
        "num_heads = None\n",
        "query = tf.keras.layers.Input(shape=(num_heads,None,3,))  \n",
        "value = tf.keras.layers.Input(shape=(num_heads,4,2,)) \n",
        "key = tf.keras.layers.Input(shape=(num_heads,4,3,))\n",
        "\n",
        "x = tf.keras.layers.Attention()([query, value, key])\n",
        "model = tf.keras.models.Model(inputs=[query, value, key], outputs=x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0tI1mcIvdXR"
      },
      "source": [
        "q = np.array([[[[0, 0, 10],     # head 1\n",
        "                [0, 10, 0],\n",
        "                [10, 10, 0]], \n",
        "               [[0, 0, 20],     # head 2\n",
        "                [0, 20, 0],\n",
        "                [10, 10, 0]]\n",
        "               ]])\n",
        "v = np.array([[[[   1,0],      # head 1\n",
        "                [  10,0],\n",
        "                [ 100,5],\n",
        "                [1000,6]],\n",
        "               [[   1, 0],     # head 2\n",
        "                [   2, 0],\n",
        "                [   3,15],\n",
        "                [   4,20]]]])\n",
        "k = np.array([[[[10,0,0],      # head 1\n",
        "                [0,10,0],\n",
        "                [0,0,10],\n",
        "                [0,0,10]],\n",
        "               [[10,0,5],      # head 2\n",
        "                [0,10,0],\n",
        "                [0,5,10],\n",
        "                [5,0,10]]]])\n",
        " \n",
        "model.predict([q,v,k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gk0dIJP6z35"
      },
      "source": [
        "For each of [query, value, key]   \n",
        "we reshape from  (batch_size , seq_length, depth) ->  (batch_size , seq_length, num_heads, multi_headed_depth)   \n",
        "then permute  ->  (batch_size , num_heads, seq_length, multi_headed_depth)  \n",
        "where multi-headed_depth = depth / num_heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNZRelVEt3zr"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model = 512, num_heads = 8, causal=False, dropout=0.0):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    assert d_model % num_heads == 0\n",
        "    depth = d_model // num_heads\n",
        "\n",
        "    self.w_query = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_query = tf.keras.layers.Reshape((-1,num_heads,depth))  \n",
        "    self.split_permute_query = tf.keras.layers.Permute((2,1,3))      \n",
        "\n",
        "    self.w_value = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_value = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
        "    self.split_permute_value = tf.keras.layers.Permute((2,1,3))\n",
        "\n",
        "    self.w_key = tf.keras.layers.Dense(d_model)\n",
        "    self.split_reshape_key = tf.keras.layers.Reshape((-1,num_heads,depth))\n",
        "    self.split_permute_key = tf.keras.layers.Permute((2,1,3))\n",
        "\n",
        "    self.attention = tf.keras.layers.Attention(causal=causal, dropout=dropout)\n",
        "    self.join_permute_attention = tf.keras.layers.Permute((2,1,3))\n",
        "    self.join_reshape_attention = tf.keras.layers.Reshape((-1,d_model))\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    q = inputs[0]\n",
        "    v = inputs[1]\n",
        "    k = inputs[2] if len(inputs) > 2 else v\n",
        "\n",
        "    query = self.w_query(q)\n",
        "    query = self.split_reshape_query(query)    \n",
        "    query = self.split_permute_query(query)                 \n",
        "\n",
        "    value = self.w_value(v)\n",
        "    value = self.split_reshape_value(value)\n",
        "    value = self.split_permute_value(value)\n",
        "\n",
        "    key = self.w_key(k)\n",
        "    key = self.split_reshape_key(key)\n",
        "    key = self.split_permute_key(key)\n",
        "\n",
        "    if mask is not None:\n",
        "      if mask[0] is not None:\n",
        "        mask[0] = tf.keras.layers.Reshape((-1,1))(mask[0])\n",
        "        mask[0] = tf.keras.layers.Permute((2,1))(mask[0])\n",
        "      if mask[1] is not None:\n",
        "        mask[1] = tf.keras.layers.Reshape((-1,1))(mask[1])\n",
        "        mask[1] = tf.keras.layers.Permute((2,1))(mask[1])\n",
        "\n",
        "    attention = self.attention([query, value, key], mask=mask)\n",
        "    attention = self.join_permute_attention(attention)\n",
        "    attention = self.join_reshape_attention(attention)\n",
        "\n",
        "    x = self.dense(attention)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnEGb28n80cO"
      },
      "source": [
        "# Encoder and Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGiHEQU8hlU"
      },
      "source": [
        "## Encoder and Decoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7KMjvg6oeZF"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.multi_head_attention =  MultiHeadAttention(d_model, num_heads)\n",
        "    self.dropout_attention = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_attention = tf.keras.layers.Add()\n",
        "    self.layer_norm_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(d_model)\n",
        "    self.dropout_dense = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_dense = tf.keras.layers.Add()\n",
        "    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    # print(mask)\n",
        "    attention = self.multi_head_attention([inputs,inputs,inputs], mask = [mask,mask])\n",
        "    attention = self.dropout_attention(attention, training = training)\n",
        "    x = self.add_attention([inputs , attention])\n",
        "    x = self.layer_norm_attention(x)\n",
        "    # x = inputs\n",
        "\n",
        "    ## Feed Forward\n",
        "    dense = self.dense1(x)\n",
        "    dense = self.dense2(dense)\n",
        "    dense = self.dropout_dense(dense, training = training)\n",
        "    x = self.add_dense([x , dense])\n",
        "    x = self.layer_norm_dense(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5hZJABBrvod"
      },
      "source": [
        "the causal = True argument for multi_head_attention1 automatically masks  future tokens in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1A3u3mkKAK1"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.multi_head_attention1 =  MultiHeadAttention(d_model, num_heads, causal = True)\n",
        "    self.dropout_attention1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_attention1 = tf.keras.layers.Add()\n",
        "    self.layer_norm_attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.multi_head_attention2 =  MultiHeadAttention(d_model, num_heads)\n",
        "    self.dropout_attention2 = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_attention2 = tf.keras.layers.Add()\n",
        "    self.layer_norm_attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.dense2 = tf.keras.layers.Dense(d_model)\n",
        "    self.dropout_dense = tf.keras.layers.Dropout(dropout)\n",
        "    self.add_dense = tf.keras.layers.Add()\n",
        "    self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    # print(mask)\n",
        "    attention = self.multi_head_attention1([inputs[0],inputs[0],inputs[0]], mask = [mask[0],mask[0]])\n",
        "    attention = self.dropout_attention1(attention, training = training)\n",
        "    x = self.add_attention1([inputs[0] , attention])\n",
        "    x = self.layer_norm_attention1(x)\n",
        "    \n",
        "    attention = self.multi_head_attention2([x, inputs[1],inputs[1]], mask = [mask[0],mask[1]])\n",
        "    attention = self.dropout_attention2(attention, training = training)\n",
        "    x = self.add_attention1([x , attention])\n",
        "    x = self.layer_norm_attention1(x)\n",
        "\n",
        "\n",
        "    ## Feed Forward\n",
        "    dense = self.dense1(x)\n",
        "    dense = self.dense2(dense)\n",
        "    dense = self.dropout_dense(dense, training = training)\n",
        "    x = self.add_dense([x , dense])\n",
        "    x = self.layer_norm_dense(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRONqwb78use"
      },
      "source": [
        "## Encoder and Decoder Blocks\n",
        "The encoder/decoder block \n",
        "\n",
        "1.   embeds the input\n",
        "2.   adds positional encoding to the embeding\n",
        "3.   adds n encoder/decoder layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zh-sfOMzLRQ"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, num_layers = 4, d_model = 512, num_heads = 8, dff = 2048, maximum_position_encoding = 10000, dropout = 0.0):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)\n",
        "    self.pos = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.encoder_layers = [ EncoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    x = self.embedding(inputs)\n",
        "    # positional encoding\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # scaling by the sqrt of d_model, not sure why or if needed??\n",
        "    x += self.pos[: , :tf.shape(x)[1], :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    #Encoder layer\n",
        "    embedding_mask = self.embedding.compute_mask(inputs)\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x, mask = embedding_mask)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return self.embedding.compute_mask(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaHXOI7UMNiQ"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, target_vocab_size, num_layers = 4, d_model = 512, num_heads = 8, dff = 2048, maximum_position_encoding = 10000, dropout = 0.0):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, mask_zero=True)\n",
        "    self.pos = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.decoder_layers = [ DecoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout)  for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs, mask=None, training=None):\n",
        "    x = self.embedding(inputs[0])\n",
        "    # positional encoding\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # scaling by the sqrt of d_model, not sure why or if needed??\n",
        "    x += self.pos[: , :tf.shape(x)[1], :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    #Decoder layer\n",
        "    embedding_mask = self.embedding.compute_mask(inputs[0])\n",
        "    for decoder_layer in self.decoder_layers:\n",
        "      x = decoder_layer([x,inputs[1]], mask = [embedding_mask, mask])\n",
        "\n",
        "    return x\n",
        "\n",
        "  # Comment this out if you want to use the masked_loss()\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return self.embedding.compute_mask(inputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOsWp_L19qm8"
      },
      "source": [
        "# Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wOZpgGc8hSH"
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(None,))\n",
        "target = tf.keras.layers.Input(shape=(None,))\n",
        "encoder = Encoder(input_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate)\n",
        "decoder = Decoder(target_vocab_size, num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, dropout = dropout_rate)\n",
        "\n",
        "x = encoder(input)\n",
        "x = decoder([target, x] , mask = encoder.compute_mask(input))\n",
        "#  tf.keras.layers.Masking ??\n",
        "x = tf.keras.layers.Dense(target_vocab_size)(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[input, target], outputs=x)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgZGAUMhQXcg"
      },
      "source": [
        "predict before train to test everying is working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlVGioSVFcAI"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "plt.pcolormesh(model.predict([pt_batch,en_batch])[5],cmap='RdBu')\n",
        "plt.colorbar()\n",
        "model.predict([pt_batch,en_batch]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nppAqokzQvdr"
      },
      "source": [
        "Custom schedule for optimizer. Works without this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB3mseCL2yn0"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    #######Heiko fix error message########\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    ##################\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    ########Heiko fix error message#######\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    ###############\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGXTseklRRQ2"
      },
      "source": [
        "Use Adam optimizer with the custom schedule above\n",
        "\n",
        "Use Sparse Categorical Crossentropy loss with the madding masked.  \n",
        "> If I knew what I was doing there should be a way to have the mask from the embedding properage though the entire model. Possible use tf.keras.layers.Masking at the end of the model?   \n",
        "Adding 'def compute_mask' to the decoder did somthing not sure if it was what I wanted. Seems to be converging correctly but the loss values are lower then I expected\n",
        "\n",
        "various different metrics to watch while fitting\n",
        "\n",
        "Complie the model with all of the above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cit0Nd8DEFjn"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "  _loss = loss(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=_loss.dtype)\n",
        "  _loss *= mask\n",
        "\n",
        "  return tf.reduce_sum(_loss)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "metrics = [loss, masked_loss, tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss = loss, metrics = metrics) # masked_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAUt8BBjMs8n"
      },
      "source": [
        "num_batches = 0\n",
        "for (batch, (_,_)) in enumerate(train_dataset):\n",
        "  num_batches = batch\n",
        "print(num_batches)\n",
        "\n",
        "val_batches = 0\n",
        "for (batch, (_,_)) in enumerate(val_dataset):\n",
        "  val_batches = batch\n",
        "print(val_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9BrnE6_m4Id"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmBFd1nUTAHw"
      },
      "source": [
        "when fitting we want input, target as inputs and target as an output. we need the target_input to be shifted 1 from the target_out.\n",
        "\n",
        "used a generator to get what we want from train_dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHU3F9_VE5rp"
      },
      "source": [
        "def generator(data_set):\n",
        "  while True:\n",
        "    for pt_batch, en_batch in data_set:\n",
        "      yield ( [pt_batch , en_batch[:, :-1] ] , en_batch[:, 1:] )\n",
        "\n",
        "\n",
        "history = model.fit(x = generator(train_dataset), validation_data = generator(val_dataset), epochs=20, steps_per_epoch = num_batches, validation_steps = val_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPg-lhA6Kt9I"
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "plt.pcolormesh(model.predict([pt_batch,en_batch])[5],cmap='RdBu')\n",
        "plt.colorbar()\n",
        "model.predict([pt_batch,en_batch]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__tSEwHm8lI"
      },
      "source": [
        "## Prediction\n",
        "When predicting input what you want to translate to the encoder and input the start token to the decoder.   \n",
        "Repeat this except use the output of the last prediction as the input to the decoder.  \n",
        "Stop when the last value of the output is the stop token.\n",
        "\n",
        "This final output (with the start and stop toekns removed ) can be fed to tokenizer_en.decode() to get an english sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKz7h4gei04o"
      },
      "source": [
        "for i in range(10):\n",
        "  translation = [tokenizer_en.vocab_size]\n",
        "  for _ in range(40):\n",
        "    predict = model.predict([pt_batch[i:i+1],np.asarray([translation])])\n",
        "    translation.append(np.argmax(predict[-1,-1]))\n",
        "    if translation[-1] == tokenizer_en.vocab_size + 1:\n",
        "      break\n",
        "\n",
        "  real_translation = []\n",
        "  for w in en_batch[:,1:][i].numpy():\n",
        "    if w == tokenizer_en.vocab_size + 1:\n",
        "      break\n",
        "    real_translation.append(w)\n",
        "  print(tokenizer_en.decode(real_translation))\n",
        "  print(tokenizer_en.decode(translation[1:-1]))\n",
        "  print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82qJvXZroziY"
      },
      "source": [
        "# Simple Transformer using only Keras Functional API\n",
        "* No Masking (\n",
        "* No Multi-Headed attention  \n",
        "* No Dropout\n",
        "* Single Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IiLFvlvlQld"
      },
      "source": [
        "# tf.python.framework_ops.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7TFAAnXklib"
      },
      "source": [
        "d_model = 512\n",
        "dff=2048\n",
        "maximum_position_encoding = 10000\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "\n",
        "scaling_factor = tf.keras.backend.constant(np.sqrt(d_model), shape = (1,1,1))\n",
        "\n",
        "# Encoder ##################################\n",
        "input = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "x = tf.keras.layers.Embedding(input_vocab_size, d_model)(input) #, mask_zero=True\n",
        "\n",
        "## positional encoding\n",
        "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
        "pos = positional_encoding(maximum_position_encoding, d_model)\n",
        "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :]] )\n",
        "\n",
        "## self-attention\n",
        "query = tf.keras.layers.Dense(d_model)(x)\n",
        "value = tf.keras.layers.Dense(d_model)(x)\n",
        "key = tf.keras.layers.Dense(d_model)(x)\n",
        "attention = tf.keras.layers.Attention()([query, value, key])                   # , mask=[query._keras_mask, value._keras_mask]\n",
        "attention = tf.keras.layers.Dense(d_model)(attention)\n",
        "\n",
        "x = tf.keras.layers.Add()([x , attention])\n",
        "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "## Feed Forward\n",
        "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
        "dense = tf.keras.layers.Dense(d_model)(dense)\n",
        "x = tf.keras.layers.Add()([x , dense])                                          # residual connection\n",
        "encoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "# Decoder ##################################\n",
        "target = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "x = tf.keras.layers.Embedding(target_vocab_size, d_model )(target) # , mask_zero=True\n",
        "\n",
        "## positional encoding\n",
        "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
        "pos = positional_encoding(maximum_position_encoding, d_model)\n",
        "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :] ])\n",
        "\n",
        "## self-attention\n",
        "query = tf.keras.layers.Dense(d_model)(x)\n",
        "value = tf.keras.layers.Dense(d_model)(x)\n",
        "key = tf.keras.layers.Dense(d_model)(x)\n",
        "attention = tf.keras.layers.Attention(causal = True)([query, value, key])       # , mask=[query._keras_mask, value._keras_mask]\n",
        "attention = tf.keras.layers.Dense(d_model)(attention)\n",
        "\n",
        "x = tf.keras.layers.Add()([x , attention])                                      # residual connection\n",
        "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "## encoder-decoder attention\n",
        "query = tf.keras.layers.Dense(d_model)(x)\n",
        "value = tf.keras.layers.Dense(d_model)(encoder)\n",
        "key = tf.keras.layers.Dense(d_model)(encoder)\n",
        "attention = tf.keras.layers.Attention()([query, value, key])                    # , mask=[query._keras_mask, value._keras_mask]\n",
        "attention = tf.keras.layers.Dense(d_model)(attention)\n",
        "\n",
        "x = tf.keras.layers.Add()([x , attention])                                      # residual connection\n",
        "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "## Feed Forward\n",
        "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
        "dense = tf.keras.layers.Dense(d_model)(dense)\n",
        "x = tf.keras.layers.Add()([x , dense])                                          # residual connection\n",
        "decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "######################################################\n",
        "\n",
        "x = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
        "\n",
        "base_model = tf.keras.models.Model(inputs=[input,target], outputs=x)\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M3CMZF1s0Sl"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "  _loss = loss(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=_loss.dtype)\n",
        "  _loss *= mask\n",
        "\n",
        "  return tf.reduce_sum(_loss)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "metrics = [loss, masked_loss, tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "base_model.compile(optimizer=optimizer, loss = loss, metrics = metrics) # masked_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjhneKhvtIi-"
      },
      "source": [
        "def generator(data_set):\n",
        "  while True:\n",
        "    for pt_batch, en_batch in data_set:\n",
        "      yield ( [pt_batch , en_batch[:, :-1] ] , en_batch[:, 1:] )\n",
        "\n",
        "def training_map(pt, en):\n",
        "  return [pt , en[:-1]] , en[1:]\n",
        "\n",
        "# def tf_gen(pt, en):\n",
        "#   input_pt, input_en, output_en = tf.py_function(gen, [pt, en], [tf.int64, tf.int64, tf.int64])\n",
        "#   input_pt.set_shape([None])\n",
        "#   input_en.set_shape([None])\n",
        "#   output_en.set_shape([None])\n",
        "#   return [input_pt, input_en], output_en\n",
        "\n",
        "\n",
        "# history = base_model.fit(x = train_dataset.map(training_map), epochs=20)\n",
        "history = base_model.fit(x = generator(train_dataset), validation_data = generator(val_dataset), epochs=20, steps_per_epoch = num_batches, validation_steps = val_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTry2D6VtRh7"
      },
      "source": [
        "for i in range(10):\n",
        "  translation = [tokenizer_en.vocab_size]\n",
        "  for _ in range(40):\n",
        "    predict = base_model.predict([pt_batch[i:i+1],np.asarray([translation])])\n",
        "    translation.append(np.argmax(predict[-1,-1]))\n",
        "    if translation[-1] == tokenizer_en.vocab_size + 1:\n",
        "      break\n",
        "\n",
        "  real_translation = []\n",
        "  for w in en_batch[:,1:][i].numpy():\n",
        "    if w == tokenizer_en.vocab_size + 1:\n",
        "      break\n",
        "    real_translation.append(w)\n",
        "  print(tokenizer_en.decode(real_translation))\n",
        "  print(tokenizer_en.decode(translation[1:-1]))\n",
        "  print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI1hJAizzM7e"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}